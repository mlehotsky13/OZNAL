dtm_train
View(vc_p_test)
load("2.RData")
ls <- meta(vc_p, "lewissplit")
ls <- unlist(ls, use.names=FALSE)
lsDt <- as.data.frame(table(ls))
View(lsDt)
cgi <- meta(vc_p, "cgisplit")
cgi <- unlist(cgi, use.names=FALSE)
cgiDt <- as.data.frame(table(cgi))
View(cgiDt)
dtm_train <- DocumentTermMatrix(vc_p_train)
dtm_test <- DocumentTermMatrix(vc_p_test)
vc_p_train <- tm_filter(vc_p, FUN = function(x){meta(x)[["lewissplit"]] == "TRAIN"})[1:5000]
vc_p_test <- tm_filter(vc_p, FUN = function(x){meta(x)[["lewissplit"]] == "TEST"})[1:2000]
# remove unnecessary objects
remove(vs, vc, vc_p, lsDt, cgiDt, topicDt)
help(stopword)
help(stopwords)
weightedtdm <- weightTfIdf(dtm_train)
dtm_train <- DocumentTermMatrix(vc_p_train)
dtm_test <- DocumentTermMatrix(vc_p_test)
weightedtdm <- weightTfIdf(dtm_train)
as.matrix(weightedtdm)[10:20,2000:2100]
findFreqTerms(dtm_train, 250)
findFreqTerms(dtm_test, 250)
weightedtdm <- as.data.frame(inspect(weightedtdm))
dtm_train
View(vc_p_test)
vc_p_train[[2]]$content
corpus <- tm_map(vc_p_train, stemDocument, language = "english")
library(tm)
corpus <- tm_map(vc_p_train, stemDocument, language = "english")
install.packages("SnowballC")
library("SnowballC")
corpus <- tm_map(vc_p_train, stemDocument, language = "english")
corpus[[2]]$content
vc_p_train[[2]]$content
corpus.final <- tm_map(corpus, stemCompletion, dictionary = vc_p_train)
help(stemCompletion)
stemCompletion(c("compan", "entit", "suppl"), crude)
corpus[[2]]$content
stemCompletion(c("manag", "activ", "financi"), corpus)
corpus[[2]]$content
stemCompletion(c("compan", "activ", "financi"), corpus)
vc_p_train[[2]]$content
corpus[[2]]$content
weightedtdm <- weightTfIdf(dtm_train)
weightedtdm
as.matrix(weightedtdm)[10:20,2000:2100]
ls <- meta(vc_p, "lewissplit")
ls <- unlist(ls, use.names=FALSE)
lsDt <- as.data.frame(table(ls))
View(lsDt)
cgi <- meta(vc_p, "cgisplit")
cgi <- unlist(cgi, use.names=FALSE)
cgiDt <- as.data.frame(table(cgi))
View(cgiDt)
dim(dtm)
load90
load("2.RData")
vc_p100 <- tm_filter(vc_p, FUN = function(x){meta(x)[["lewissplit"]] == "TRAIN"})
vc_p100 <- tm_filter(vc_p, FUN = function(x){meta(x)[["lewissplit"]] == "TRAIN"})
library(tm)
vc_p100 <- tm_filter(vc_p, FUN = function(x){meta(x)[["lewissplit"]] == "TRAIN"})
# creating Document Term Matrix
dtm <- DocumentTermMatrix(vc_p100)
dim(dtm)
as.matrix(dtm)[10:20,200:210]
vc_p100 <- tm_filter(vc_p, FUN = function(x){meta(x)[["lewissplit"]] == "TRAIN"})[[1:5000]]
vc_p100 <- tm_filter(vc_p, FUN = function(x){meta(x)[["lewissplit"]] == "TRAIN"})(1:5000)
vc_p100 <- vc_p100[[1:100]]
vc_p100 <- tm_filter(vc_p, FUN = function(x){meta(x)[["lewissplit"]] == "TRAIN"})[1:100]
# creating Document Term Matrix
dtm <- DocumentTermMatrix(vc_p100)
dim(dtm)
weightedtdm <- weightTfIdf(dtm)
View(weightedtdm)
as.matrix(weightedtdm)[10:20,200:210]
summary(vc)
inspect(dtm[1:10,1:10])
# creating Document Term Matrix
tdm <- TermDocumentMatrix(vc_p100)
dim(tdm)
inspect(tdm[1:10,1:10])
dim(tdm)
inspect(tdm[1:10,1:10])
findFreqTerms(tdm, 100)
findAssocs(tdm, "mln", 0.8)
findAssocs(tdm, "mln", 0.5)
findFreqTerms(tdm, 50)
findAssocs(tdm, "said", 0.5)
findAssocs(tdm, "said", 0.8)
findAssocs(tdm, "said", 0.7)
findFreqTerms(tdm, 10)
help("findFreqTerms")
findFreqTerms(tdm, highfreq = 1)
findFreqTerms(tdm, highfreq = 0)
findFreqTerms(tdm, highfreq = 1)
help(removeSparseTerms)
tdm_p <- removeSparseTerms(tdm, 0.2)
dim(tdm)
dim(tdm_p)
tdm_p <- removeSparseTerms(tdm, 0.1)
dim(tdm_p)
tdm_p <- removeSparseTerms(tdm, 0.5)
dim(tdm_p)
tdm_p <- removeSparseTerms(tdm, 0.8)
dim(tdm_p)
inspect(tdm[1:10,1:10])
tdm_p <- removeSparseTerms(tdm, 0.8)
dim(tdm_p)
inspect(tdm_p)
library(reshape2)
inspect(tdm[1:10,1:10])
findAssocs(tdm, "said", 0.7)
findFreqTerms(tdm, 50)
findAssocs(tdm, "said", 0.7)
help(findAssocs)
load("2.RData")
remove(vc, vs)
vc_p100 <- tm_filter(vc_p, FUN = function(x){meta(x)[["lewissplit"]] == "TRAIN"})[1:100]
library(tm)
vc_p100 <- tm_filter(vc_p, FUN = function(x){meta(x)[["lewissplit"]] == "TRAIN"})[1:100]
View(vc_p100)
# creating Term Document Matrix
tdm <- TermDocumentMatrix(vc_p100)
View(tdm)
tdm
inspect(tdm)
dim(tdm)
inspect(tdm[1:10,1:10])
inspect(tdm)
help(inspect)
inspect(tdm)
help(colnames)
colnames(tdm)[200:210]
colnames(tdm)[1:100]
vc_p100[[2]]$content
vc_p100[[5]]$content
vc_p100[[10]]$content
getTransformations()
help("removeWords")
help(stopwords)
help("stemCompletion")
View(vc_p100)
View(vc_p)
vc_p <- tm_map(vc_p, PlainTextDocument)
View(vc_p)
load("2.RData")
library(tm)
# remove VectorSource and VCorpus because they are no longer needed
remove(vc, vs)
# transform content of documents by removal of multispaces
vc_p <- tm_map(vc_p, stripWhitespace)
vc_p <- tm_map(vc_p, PlainTextDocument)
View(vc_p)
load("2.RData")
# remove VectorSource and VCorpus because they are no longer needed
remove(vc, vs)
View(vc_p)
help(colSums)
# creating Term Document Matrix
tdm <- TermDocumentMatrix(vc_p100)
vc_p100 <- tm_filter(vc_p, FUN = function(x){meta(x)[["lewissplit"]] == "TRAIN"})[1:100]
# creating Term Document Matrix
tdm <- TermDocumentMatrix(vc_p100)
dim(tdm)
inspect(tdm[1:10, 1:20])
inspect(tdm[1:5, 1:20])
inspect(tdm[1:5, 1:30])
inspect(tdm[1:5, 1:20])
dim(tdm)
inspect(tdm[1:10,1:10])
freq <- colSums(as.matrix(tdm))
length(freq)
typeof(freq)
dim(freq)
inspect(freq)
colSums
help(colSums)
help(rowSums)
inspect(tdm[1:10,1:10])
freq <- rowSums(tdm[1:10, 1:10])
tdmMatrix <- as.matrix(tdm)
View(tdmMatrix)
View(tdm)
tdmMatrix <- as.matrix(tdm[1:10, 1:10])
View(tdm)
View(tdmMatrix)
freq <- rowSums(tdm[100:110, 1:10])
tdmMatrix <- as.matrix(tdm[100:110, 1:10])
View(tdmMatrix)
freq <- rowSums(tdm)
freq <- rowSums(tdmMatrix)
length(freq)
freq
help(order)
order(freq)
tdmMatrix <- as.matrix(tdm)
freq <- rowSums(tdmMatrix)
freq
tdmMatrix <- as.matrix(tdm[100:110, 1:10])
freq <- rowSums(tdmMatrix)
freq
order(freq)
freq
order(freq)
help(order)
tdmMatrix <- as.matrix(tdm[100:110, 1:10])
freq <- rowSums(tdmMatrix)
freq
typeof(freq)
tdmMatrix <- as.matrix(tdm[100:110, 1:10])
freq <- rowSums(tdmMatrix)
freq
help(order)
help(sort.list)
sort.list(freq)
tdm_p <- removeSparseTerms(tdm, 0.2)
inspect(tdm_p)
dim(tdm_p)
inspect(tdm_p)
tdm_p <- removeSparseTerms(tdm, 0.5)
inspect(tdm_p)
tdm_p <- removeSparseTerms(tdm, 0.8)
inspect(tdm_p)
vc_p100 <- tm_filter(vc_p, FUN = function(x){meta(x)[["lewissplit"]] == "TRAIN"})[1:1000]
load("2.RData")
# remove VectorSource and VCorpus because they are no longer needed
remove(vc, vs)
vc_p100 <- tm_filter(vc_p, FUN = function(x){meta(x)[["lewissplit"]] == "TRAIN"})[1:1000]
# creating Term Document Matrix
tdm <- TermDocumentMatrix(vc_p100)
dim(tdm)
inspect(tdm[1:10,1:10])
tdmMatrix <- as.matrix(tdm)
dim(tdm)
dim(tdmMatrix)
inspect(tdmMatrix)
tdmMatrix
freq <- rowSums(tdmMatrix)
freq
head(table(freq), 20)
tail(table(freq), 20)
tail(table(freq), 50)
load("2.RData")
# remove VectorSource and VCorpus because they are no longer needed
remove(vc, vs)
vc_p100 <- tm_filter(vc_p, FUN = function(x){meta(x)[["lewissplit"]] == "TRAIN"})[1:1000]
# creating Term Document Matrix
tdm <- TermDocumentMatrix(vc_p100)
dim(tdm)
inspect(tdm[1:10,1:10])
tdmMatrix <- as.matrix(tdm)
freq <- sort(rowSums(tdmMatrix), decreasing=TRUE)
head(freq, 14)
tdmMatrix <- as.matrix(tdm[1:10, 1:10])
tdmMatrix
tdmMatrix
View(tdmMatrix)
tdmMatrix <- as.matrix(tdm)
freq <- sort(rowSums(tdmMatrix), decreasing=TRUE)
freq
freq <- rowSums(tdmMatrix)
freq
freq <- sort(rowSums(tdmMatrix), decreasing=TRUE)
freq
wf <- data.frame(word=names(freq), freq=freq)
head(wf)
head(freq, 15)
library(ggplot2)
install.packages(ggplot2)
install.packages("ggplot2")
library(ggplot2)
head(wf)
head(wf)
wf <- data.frame(freq=freq)
head(wf)
wf <- data.frame()
head(wf)
wf <- data.frame(freq=freq)
head(wf)
head(wf, 30)
p <- ggplot(subset(wf, freq>50), aes(x = reorder(word, -freq), y = freq)) +
geom_bar(stat = "identity") +
theme(axis.text.x=element_text(angle=45, hjust=1))
p
p <- ggplot(subset(wf, freq>50), aes(x = reorder(word, -freq), y = freq)) +
geom_bar(stat = "identity") +
theme(axis.text.x=element_text(angle=45, hjust=1))
p
p <- ggplot(subset(wf, freq>50), aes(x = reorder(-freq), y = freq)) +
geom_bar(stat = "identity") +
theme(axis.text.x=element_text(angle=45, hjust=1))
p
wf <- data.frame(word=names(freq), freq=freq)
p <- ggplot(subset(wf, freq>50), aes(x = reorder(word, -freq), y = freq)) +
geom_bar(stat = "identity") +
theme(axis.text.x=element_text(angle=45, hjust=1))
p
p <- ggplot(subset(wf, freq>100), aes(x = reorder(word, -freq), y = freq)) +
geom_bar(stat = "identity") +
theme(axis.text.x=element_text(angle=45, hjust=1))
p
help(reorder)
load("2.RData")
summary(vc)
topic <- meta(vc, "topics")
topic <- unlist(topic, use.names=FALSE)
topicDt <- as.data.frame(table(topic))
View(topicDt)
ls <- meta(vc, "lewissplit")
ls <- unlist(ls, use.names=FALSE)
lsDt <- as.data.frame(table(ls))
View(lsDt)
cgi <- meta(vc, "cgisplit")
cgi <- unlist(cgi, use.names=FALSE)
cgiDt <- as.data.frame(table(cgi))
View(cgiDt)
# remove VectorSource and VCorpus because they are no longer needed
remove(vc, vs)
library(tm)
library(ggplot2)
processFile <- function(filepath) {
con = file(filepath, "r")
resultVector <- c()
x <- character()
while ( length(line) != 0 ) {
line = readLines(con, n = 1, encoding = "latin1")
line = gsub("&#\\d+;", "", line)
x = paste(x, line)
if ( length(line) == 0 || (length(line) != 0 && grepl("</REUTERS>", line))){
resultVector <- union(resultVector, c(x))
x <- character()
}
}
close(con)
return(resultVector)
}
files <- list.files(path="Dataset/reuters21578/test", pattern="*.sgm", full.names=T, recursive=FALSE)
allDocs <- c()
for (i in 1:length(files)){
docsOfFile <- processFile(files[i])
allDocs <- union(allDocs, docsOfFile)
}
vs <- VectorSource(allDocs)
vc <- VCorpus(vs, readerControl = list(reader = readReut21578XMLasPlain))
removeMetaAttributes <- function(x){
PlainTextDocument(x,
id = meta(x, "id"),
topics = meta(x, "topics"),
topics_cat = meta(x, "topics_cat"),
lewissplit = meta(x, "lewissplit"),
cgisplit = meta(x, "cgisplit"))
}
removeMetaAttributes <- function(x){
PlainTextDocument(x,
id = meta(x, "id"),
topics = meta(x, "topics"),
topics_cat = meta(x, "topics_cat"),
lewissplit = meta(x, "lewissplit"),
cgisplit = meta(x, "cgisplit"))
}
vc <- tm_map(vc, removeMetaAttributes)
topic <- meta(vc, "topics")
topic <- unlist(topic, use.names=FALSE)
topicDt <- as.data.frame(table(topic))
ls <- meta(vc, "lewissplit")
ls <- unlist(ls, use.names=FALSE)
lsDt <- as.data.frame(table(ls))
cgi <- meta(vc, "cgisplit")
cgi <- unlist(cgi, use.names=FALSE)
cgiDt <- as.data.frame(table(cgi))
vc_p <- vc
# transform content of documents to lower case
vc_p <- tm_map(vc_p, content_transformer(tolower))
# transform content of documents by removal of numbers
vc_p <- tm_map(vc_p, removeNumbers)
# transform content of documents by removal of punctuation
vc_p <- tm_map(vc_p, removePunctuation)
# transform content of documents by removal of stopwords
vc_p <- tm_map(vc_p, removeWords, stopwords("en"))
# transform content of documents by removal of multispaces
vc_p <- tm_map(vc_p, stripWhitespace)
# remove unused structures
remove(cgiDt, lsDt, topicDt, vc, vs)
# get subset corpus of train documents
vc_p1000 <- tm_filter(vc_p, FUN = function(x){meta(x)[["lewissplit"]] == "TRAIN"})[1:1000]
tdm <- TermDocumentMatrix(vc_p1000)
dim(tdm)
inspect(tdm[1:10,1:10])
# creating matrix
tdmMatrix <- as.matrix(tdm)
freq <- sort(rowSums(tdmMatrix), decreasing=TRUE)
wf <- data.frame(word=names(freq), freq=freq)
head(wf, 20)
p <- ggplot(subset(wf, freq>100), aes(x = reorder(word, -freq), y = freq)) +
geom_bar(stat = "identity") +
theme(axis.text.x=element_text(angle=45, hjust=1))
p
load("2.RData")
# remove word 'reuter' at the end of content
vc_p <- tm_map(vc_p, removeWords, c("reuter"))
# transform content of documents by removal of multispaces
vc_p <- tm_map(vc_p, stripWhitespace)
vc_p[[1]]$content
load("2.RData")
vc_p[[1]]$content
)
# transform content of documents by removal of multispaces
vc_p <- tm_map(vc_p, stripWhitespace)
vc_p <- tm_map(vc_p, removeWords, c("reuter"))
# transform content of documents by removal of multispaces
vc_p <- tm_map(vc_p, stripWhitespace)
vc_p[[1]]$content
load("2.RData")
vc_p[[1]]$content
vc_p <- tm_map(vc_p, removeWords, c("reuter"))
# transform content of documents by removal of multispaces
vc_p <- tm_map(vc_p, stripWhitespace)
vc_p[[1]]$content
library(tm)
library(ggplot2)
# function for processing documents inside file
processFile <- function(filepath) {
con = file(filepath, "r")
resultVector <- c()
x <- character()
while ( length(line) != 0 ) {
line = readLines(con, n = 1, encoding = "latin1")
line = gsub("&#\\d+;", "", line)
x = paste(x, line)
if ( length(line) == 0 || (length(line) != 0 && grepl("</REUTERS>", line))){
resultVector <- union(resultVector, c(x))
x <- character()
}
}
close(con)
return(resultVector)
}
# going through every .sgm file in given directory
files <- list.files(path="Dataset/reuters21578/test", pattern="*.sgm", full.names=T, recursive=FALSE)
allDocs <- c()
for (i in 1:length(files)){
docsOfFile <- processFile(files[i])
allDocs <- union(allDocs, docsOfFile)
}
# creating corpus
vs <- VectorSource(allDocs)
vc <- VCorpus(vs, readerControl = list(reader = readReut21578XMLasPlain))
# removing unnecessary meta attributes
removeMetaAttributes <- function(x){
PlainTextDocument(x,
id = meta(x, "id"),
topics = meta(x, "topics"),
topics_cat = meta(x, "topics_cat"),
lewissplit = meta(x, "lewissplit"),
cgisplit = meta(x, "cgisplit"))
}
vc <- tm_map(vc, removeMetaAttributes)
# observe documents with and without set topic
topic <- meta(vc, "topics")
topic <- unlist(topic, use.names=FALSE)
topicDt <- as.data.frame(table(topic))
# observe splits to train and test sets according to lewis
ls <- meta(vc, "lewissplit")
ls <- unlist(ls, use.names=FALSE)
lsDt <- as.data.frame(table(ls))
# observe splits to train and test sets according to cgi
cgi <- meta(vc, "cgisplit")
cgi <- unlist(cgi, use.names=FALSE)
cgiDt <- as.data.frame(table(cgi))
# PREPROCESSING of text contents
vc_p <- vc
# transform content of documents to lower case
vc_p <- tm_map(vc_p, content_transformer(tolower))
# transform content of documents by removal of numbers
vc_p <- tm_map(vc_p, removeNumbers)
# transform content of documents by removal of punctuation
vc_p <- tm_map(vc_p, removePunctuation)
# transform content of documents by removal of stopwords
vc_p <- tm_map(vc_p, removeWords, stopwords("en"))
# remove word 'reuter' at the end of content
vc_p <- tm_map(vc_p, removeWords, c("reuter"))
# transform content of documents by removal of multispaces
vc_p <- tm_map(vc_p, stripWhitespace)
save.image("~/Desktop/SKOLA/Ing/2. semester/Objavovanie znalosti/Projekt/R/3.RData")
# remove unused structures
remove(cgiDt, lsDt, topicDt, vc, vs)
load(3.RData)
load("3.RData)
;
quit
exit
\q
load("3.RData")
# remove unused structures
remove(allDocs, cgi, docsOfFile, files, i, ls, topic, cgiDt, lsDt, topicDt, vc, vs)
# get subset corpus of train documents
vc_p1000 <- tm_filter(vc_p, FUN = function(x){meta(x)[["lewissplit"]] == "TRAIN"})[1:1000]
# creating Term Document Matrix
tdm <- TermDocumentMatrix(vc_p1000)
dim(tdm)
# creating matrix
tdmMatrix <- as.matrix(tdm)
freq <- sort(rowSums(tdmMatrix), decreasing=TRUE)
wf <- data.frame(word=names(freq), freq=freq)
p <- ggplot(subset(wf, freq>100), aes(x = reorder(word, -freq), y = freq)) +
geom_bar(stat = "identity") +
theme(axis.text.x=element_text(angle=45, hjust=1))
p
